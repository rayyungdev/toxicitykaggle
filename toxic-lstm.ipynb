{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35203a5e",
   "metadata": {},
   "source": [
    "# Ranking Toxic Comments using LSTM network maybe?\n",
    "\n",
    "So while ray is trying the more efficient approach of using naive-bayes classification, I'd try training an entire LSTM to solve this problem... in jupyter notebook... so uh.. yeah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "276b2cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, GRU, Dense, Dropout\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d067aff7",
   "metadata": {},
   "source": [
    "My plan is to use two LSTM networks, to encode both comments and output a one-hot vector with 2 values, one for if it's greater and one for if it's less. Given that the dataset only identifies if one is greater than the other, we can only make determinations based on that, so the network reflects that\n",
    "\n",
    "Something like this:\n",
    "\n",
    "![Toxicity LSTM Network](toxicity-lstm-network.svg)\n",
    "\n",
    "I think the size of each layer can be determined by some hyperparameter tuning (or just guess & check since I'm not google and don't have infinite compute power)\n",
    "\n",
    "But first we need to download and extract the data. This requires `unzip`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9009b338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /Users/surenderkharbanda/.kaggle/kaggle.json'\n",
      "Downloading jigsaw-toxic-severity-rating.zip to /Users/surenderkharbanda/Documents/GitHub/toxicitykaggle/data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6.72M/6.72M [00:00<00:00, 16.3MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Archive:  jigsaw-toxic-severity-rating.zip\n",
      "  inflating: comments_to_score.csv   \n",
      "  inflating: sample_submission.csv   \n",
      "  inflating: validation_data.csv     \n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Download and extract data (run if on Linux/MacOS)\n",
    "rm -rf data\n",
    "mkdir data\n",
    "cd data\n",
    "kaggle competitions download --force -c jigsaw-toxic-severity-rating\n",
    "unzip jigsaw-toxic-severity-rating.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47edf9f",
   "metadata": {},
   "source": [
    "Now that we have our data, we can create a function to pull it and convert it into a tensorflow dataset for our model. The worker is the id of the person that scored the comments. They're not really important so we can ignore that column. So for each row, we'll need the row as is and also the reverse of the row (a,b and b,a). This way we can train the model to understand both greater than and less than cases. Otherwise all of these outputs will be just 1. Prior experience has taught me that word based encoding is not all that useful. We'll encode using characters. We can do this using `tf.keras.layers.StringLookup`. This will learn the best way to encode our characters by analyzing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "18583bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pulling data...\n",
      "Generating labeled data...\n",
      "Encoding inputs...\n",
      "Vocab size: 475\n",
      "Encoding labels...\n",
      "Creating dataset...\n",
      "Dataset spec: <RepeatDataset shapes: (((None, None), (None, None)), (None, 2)), types: ((tf.int64, tf.int64), tf.float64)>\n",
      "Sample input sequence A: tf.Tensor(\n",
      "[ 1 63  5  7 13  5 11  6  8 16  1 28 28 23  1  8 12 17 17  2  8  3  1 15\n",
      "  4 12  1 17  2  3  1  8  4 16  2  1  2 13 12 14  5  3  6  4  7  1 22  2\n",
      " 19  4  9  2  1 15  4 12  1  5 14  3  1  5  8  1  5  1 20  6 24  6 18  2\n",
      " 13  6  5  1  2 13  6  3  4  9 21  1 25  5  7 13  5 11  6  8 16  1 10  5\n",
      "  8  1  3 10  2  1  6  7  3  2  7  3  6  4  7  1  4 19  1 13  2  8  3  9\n",
      "  4 15  6  7 17 21 28 28 23  3  8  1  6  3  1 14  4  7  8  3  9 12 14  3\n",
      "  6 25  2  1  3  4  1 20  5  9  7  1 18  2  4 18 11  2  1  4 19 19  1 22\n",
      "  2  6  7 17  1  5  1  9  2 25  2  9  3  1 17  4  4  7  1 51 28 28 22 12\n",
      "  3  1  3 10  5  3  8  1  1 20 10  5  3  1  1  3 10  2  1  9  5  7 24  8\n",
      "  1  4 19  1 20  6 24  6 18  2 13  6  5  1  5  9  2  1 19 12 11 11  1  4\n",
      " 19 26  1  8  4  1 23  1 17 12  2  8  8  1 15  4 12  1 14  5  7 39  3  1\n",
      "  3  2 11 11  1  5  1  9  2 25  2  9  3  1 17  4  4  7  1 20 10  2  7  1\n",
      " 15  4 12  1  8  2  2  1  4  7  2 21], shape=(300,), dtype=int64)\n",
      "Sample input sequence B: tf.Tensor(\n",
      "[ 1 23 31  1 29 38 23 31  1 27  1 62 34 49 33 51  1  1 28 28 29 10  6  8\n",
      "  1  5  9  3  6 14 11  2  1  6  8  1  8  4  1 22  6  5  8  2 13  1  5 17\n",
      "  5  6  7  8  3  1 43  9  1 50  5 14 43  4  7  5 11 13  1  3 10  5  3  1\n",
      "  7  4  1  4  7  2  1 20  6 11 11  1 22  2 11  6  2 25  2  1  6  7  1  3\n",
      " 10  2  1 14  9  6  3  6 14  8 16  1 13  6  9  2 14  3  2 13  1  5  3  1\n",
      " 10  6 16 21], shape=(124,), dtype=int64)\n",
      "Sample output labels: tf.Tensor([1. 0.], shape=(2,), dtype=float64)\n",
      "Splitting into training and testing...\n"
     ]
    }
   ],
   "source": [
    "def input_data(train_frac=0.7, shuffle=200, batch=200, repeat=3, display=False):\n",
    "    \"\"\"\n",
    "    Extract and preprocess data for trainer\n",
    "    \n",
    "    :param shuffle: size of groups to shuffle rows in\n",
    "    :param batch: size of batches to segment data into\n",
    "    :param repeat: number of times to repeat dataset\n",
    "    :param display: if true, print a sample of data\n",
    "    \"\"\"\n",
    "    # Pull data from csv\n",
    "    print('Pulling data...')\n",
    "    csv_data = pd.read_csv('data/validation_data.csv')\n",
    "    csv_data = csv_data[['less_toxic', 'more_toxic']]\n",
    "    \n",
    "    # Our inputs are labeled as \"more toxic\" and \"less toxic\"\n",
    "    # but we want to pass in both with their comparision being\n",
    "    # unknown, as the network is supposed to figure that out.\n",
    "    # So, we create two sets, one of which has the order swapped\n",
    "    # and we name both sequence columns as \"sequence A\" and \n",
    "    # \"sequence B\" We then assign a label to each, with the \n",
    "    # original having 'greater' and the swapped one having 'less'. \n",
    "    # Therefore, the network will see both cases for each input \n",
    "    # in the dataset and can train for both\n",
    "    print('Generating labeled data...')\n",
    "    labeled_data_greater = csv_data.copy()\n",
    "    labeled_data_greater.rename(\n",
    "        columns={'less_toxic': 'seq_a', 'more_toxic': 'seq_b' }, \n",
    "        inplace=True)\n",
    "    labeled_data_greater['label'] = 'greater'\n",
    "    labeled_data_less = csv_data.copy()\n",
    "    labeled_data_less.rename(\n",
    "        columns={ 'more_toxic': 'seq_a', 'less_toxic': 'seq_b' }, \n",
    "        inplace=True)\n",
    "    labeled_data_less['label'] = 'less'\n",
    "    labeled_data = pd.concat([ labeled_data_greater, labeled_data_less ])\n",
    "    labeled_data = labeled_data.sample(frac=1)\n",
    "    \n",
    "    # Now we take all sequences of characters and convert them to sequences \n",
    "    # of integers. We can do that using keras's StringLookup preprocessing \n",
    "    # layer, which will take a string and spit out an array of integers \n",
    "    # encoding the characters of the string. This layer will need to scan \n",
    "    # the dataset to determine the appropriate encoding vocabulary for the \n",
    "    # characters. Since both sequences essentially contain all of the data, \n",
    "    # we can just use one of the sequences for the StringLookup to scan.\n",
    "    print('Encoding inputs...')\n",
    "    seq_a = tf.strings.unicode_split(labeled_data['seq_a'], input_encoding='UTF-8')\n",
    "    seq_b = tf.strings.unicode_split(labeled_data['seq_b'], input_encoding='UTF-8')\n",
    "    encoder = tf.keras.layers.StringLookup()\n",
    "    encoder.adapt(seq_a)\n",
    "    vocab_size = encoder.vocabulary_size()\n",
    "    if display:\n",
    "        print('Vocab size:', vocab_size)\n",
    "    seqint_a = encoder(seq_a)\n",
    "    seqint_b = encoder(seq_b)\n",
    "    \n",
    "    # Then we can one-hot encode our labels using scikit-learn's \n",
    "    # OneHotEncoder class. Since we know our labels ahead of time, \n",
    "    # I figured we don't need to train it. HOWEVER, scikit-learn \n",
    "    # doesn't seem to think so, as it expects us to call fit on \n",
    "    # the data anyway... so yeah.\n",
    "    print('Encoding labels...')\n",
    "    label_encoder = OneHotEncoder(\n",
    "        categories=[['greater', 'less']], \n",
    "        handle_unknown='ignore')\n",
    "    label_array = labeled_data['label'].values.reshape(-1, 1)\n",
    "    label_encoder.fit(label_array)\n",
    "    labels = label_encoder.transform(label_array)\n",
    "    labels = tf.constant(labels.toarray())\n",
    "    \n",
    "    # Create dataset. Shuffle, batch, repeat, etc.\n",
    "    print('Creating dataset...')\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(\n",
    "        ((seqint_a, seqint_b), labels))\n",
    "    dataset = dataset.shuffle(shuffle)\n",
    "    dataset = dataset.batch(batch)\n",
    "    dataset = dataset.repeat(repeat)\n",
    "    \n",
    "    # Display a sample\n",
    "    if display:\n",
    "        print('Dataset spec:', dataset)\n",
    "        for (seq_a, seq_b), output in dataset.take(1):\n",
    "            print(f'Sample input sequence A:', seq_a[0])\n",
    "            print(f'Sample input sequence B:', seq_b[0])\n",
    "            print(f'Sample output labels:', output[0])\n",
    "            \n",
    "    # Split into training and testing data\n",
    "    print('Splitting into training and testing...')\n",
    "    train_num = int(train_frac*len(dataset))\n",
    "    train_dataset = dataset.take(train_num)\n",
    "    test_dataset = dataset.skip(train_num)\n",
    "    \n",
    "    # Return training data, testing data, and vocab size\n",
    "    return train_dataset, test_dataset, vocab_size\n",
    "    \n",
    "# Run input data function to test it out\n",
    "train_dataset, test_dataset, vocab_size = input_data(display=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c30a7a",
   "metadata": {},
   "source": [
    "After this. We build the model using Keras's framework, train it and then validate it on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4b5252",
   "metadata": {},
   "source": [
    "The original problem calls for ranking comments based on toxicity, so we can use this network as a comparator function to sort the list of toxic comments. So, first, the network. I'm creating a function which would return a model based on parameters. This will be used for the optimization step.\n",
    "\n",
    "In the last minute, I decided that, instead of an LSTM network, I would be using a GRU network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7b1ab5d8-fb8d-4677-941b-d67f97cbe93d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_a (InputLayer)           [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " input_b (InputLayer)           [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " embed_a (Embedding)            (None, None, 64)     30400       ['input_a[0][0]']                \n",
      "                                                                                                  \n",
      " embed_b (Embedding)            (None, None, 64)     30400       ['input_b[0][0]']                \n",
      "                                                                                                  \n",
      " recur_a (GRU)                  (None, 64)           24960       ['embed_a[0][0]']                \n",
      "                                                                                                  \n",
      " recur_b (GRU)                  (None, 64)           24960       ['embed_b[0][0]']                \n",
      "                                                                                                  \n",
      " drop_a (Dropout)               (None, 64)           0           ['recur_a[0][0]']                \n",
      "                                                                                                  \n",
      " drop_b (Dropout)               (None, 64)           0           ['recur_b[0][0]']                \n",
      "                                                                                                  \n",
      " tf.concat (TFOpLambda)         (None, 128)          0           ['drop_a[0][0]',                 \n",
      "                                                                  'drop_b[0][0]']                 \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 64)           8256        ['tf.concat[0][0]']              \n",
      "                                                                                                  \n",
      " drop_d (Dropout)               (None, 64)           0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " labels (Dense)                 (None, 2)            130         ['drop_d[0][0]']                 \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 119,106\n",
      "Trainable params: 119,106\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def create_model(vocab_size, embed_units=64, recur_units=64, dense_units=64, dropout_rate=0.1):\n",
    "    \"\"\"\n",
    "    Create model using parameters\n",
    "    \"\"\"\n",
    "    # A LSTM network\n",
    "    input_a = Input((None,), name='input_a')\n",
    "    embed_a = Embedding(vocab_size, embed_units, name='embed_a')(input_a)\n",
    "    recur_a = GRU(recur_units, name='recur_a')(embed_a)\n",
    "    drop_a = Dropout(dropout_rate, name='drop_a')(recur_a)\n",
    "    \n",
    "    # B LSTM network\n",
    "    input_b = Input((None,), name='input_b')\n",
    "    embed_b = Embedding(vocab_size, embed_units, name='embed_b')(input_b)\n",
    "    recur_b = GRU(recur_units, name='recur_b')(embed_b)\n",
    "    drop_b = Dropout(dropout_rate, name='drop_b')(recur_b)\n",
    "    \n",
    "    # Concatenation and dense layers\n",
    "    concat = tf.concat([ drop_a, drop_b ], axis=1, name='concatenate')\n",
    "    dense = Dense(dense_units, activation='relu', name='dense')(concat)\n",
    "    drop_d = Dropout(dropout_rate, name='drop_d')(dense)\n",
    "    output = Dense(2, activation='relu', name='labels')(drop_d) # 2 labels in the output layer\n",
    "    \n",
    "    # Final model configuration\n",
    "    model = Model([input_a, input_b], output)\n",
    "    model.summary()\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Horay model created\n",
    "tf.keras.backend.clear_session()\n",
    "model = create_model(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b16b7d-7dca-4a6a-afc3-e60f97a49b6b",
   "metadata": {},
   "source": [
    "Now we fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4944555a-7caf-46e4-bfe4-408bf002d98b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/surenderkharbanda/Documents/GitHub/toxicitykaggle/env/lib/python3.9/site-packages/tensorflow/python/framework/indexed_slices.py:448: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/model/recur_a/RaggedToTensor/boolean_mask_1/GatherV2:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/model/recur_a/RaggedToTensor/boolean_mask/GatherV2:0\", shape=(None, 64), dtype=float32), dense_shape=Tensor(\"gradient_tape/model/recur_a/RaggedToTensor/Shape:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/Users/surenderkharbanda/Documents/GitHub/toxicitykaggle/env/lib/python3.9/site-packages/tensorflow/python/framework/indexed_slices.py:448: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/model/recur_b/RaggedToTensor/boolean_mask_1/GatherV2:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/model/recur_b/RaggedToTensor/boolean_mask/GatherV2:0\", shape=(None, 64), dtype=float32), dense_shape=Tensor(\"gradient_tape/model/recur_b/RaggedToTensor/Shape:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  6/634 [..............................] - ETA: 1:43:49 - loss: nan - accuracy: 0.4842"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function ScopedTFGraph.__del__ at 0x112e36f70>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/surenderkharbanda/Documents/GitHub/toxicitykaggle/env/lib/python3.9/site-packages/tensorflow/python/framework/c_api_util.py\", line 58, in __del__\n",
      "    self.deleter(self.graph)\n",
      "KeyboardInterrupt: \n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.fit(train_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
