{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58cd8e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4b9082",
   "metadata": {},
   "source": [
    "# What we know:\n",
    "\n",
    "#### The kaggle provided dataset contains 3 files:  \n",
    "##### comment_to_score.csv\n",
    "    - Contains all unranked comments. Our objective is to rank these comments based off the toxicity, with 1 being the most toxic. \n",
    "\n",
    "##### sample_submission.csv\n",
    "    - Contains an example of how the submission should look like... Not sure if this will help with training?\n",
    "\n",
    "##### validation_data.csv\n",
    "    - According to kaggle this is the \"pair rankings that can be used to validate models. Includes annotator work id and how the annotator ranked a given pair of comments\"\n",
    "    \n",
    "# Understanding the objective\n",
    "Before moving on, let's take a moment and understand what our overall objective is: \n",
    "We want to rank a list of comments base off of the level of toxicity without the use of any provided training data. However, while we do not have training data, we have provided validation data, which includes a set of toxicity rankings that might help to validate models.\n",
    "\n",
    "# Understanding our data:\n",
    "In the next few lines, we will import our data and see what we're actually dealing with and see if the data provided is actually useful. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec7c140",
   "metadata": {},
   "source": [
    "#### comments_to_score.csv\n",
    "        - We knew how the data looked like before, but let's take a closer look and see what might need to be cleaned up  \n",
    "   \n",
    "   - Column Names\n",
    "       - comment_id (comment identifier)\n",
    "       - text (actual text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "736a5a8c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\n",
      " \n",
      "\n",
      "Gjalexei, you asked about whether there is an \"\"anti-editorializing\"\" policy here.  There is, and it's called wikipedia:neutral point of view.  It discusses at some length  the case of what we should do when writing about a subject which most of us find repugnant.  Whilst you're not likely to get too many defenders of FGM here, the need for the policy should be clearer for articles like abortion, for instance.\n",
      "\n",
      "If something you write is edited and you're not sure why, please continue to question such edits on the talk page.  Sometimes, you'll learn more about wikipedia policy.  Sometimes, you'll find out that some other people working on here can get it flat-out wrong ) Robert Merkel\"\n"
     ]
    }
   ],
   "source": [
    "# comments_to_score data\n",
    "df = pd.read_csv(\"./data/comments_to_score.csv\")\n",
    "eg1 = df.text[0]\n",
    "print(eg1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c785209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\n",
      "\n",
      "I already gave you the source for all my edits   The Slur database, so I will not continue to play your little game. If you weren't so lazy and intent on harassment, you could use Google to search for \"\"Gargamel\"\" and \"\"Jew\"\". You get more than 400 hits including white supremacist sites and an academic paper dating to 1996. It's obviously a real slur with some usage. Nice try at being obdurate though. I'm sure there's a slur that describes that characteristic. Shyster doesn't quite cover it.  22:08, 22 Dec 2004 (UTC)\"\n"
     ]
    }
   ],
   "source": [
    "eg2 = df.text[25]\n",
    "print(eg2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057f5ce9",
   "metadata": {},
   "source": [
    "##### took two comments from the provided dataset. We can make a few observations based off of the content of the texts:   \n",
    "  \n",
    "- First, punctuation. Normally, when we deal with datasets that involve texts, we would ignore punctuation. Howevever, knowing the habits of the internet, the punctuation can reflect toxicity. For instance, quotation marks have been used to represent sarcasm.\n",
    "    -Building onto this, this same concept applies with capitalized letters. Generally speaking, when dealing with texts, we would try to make the letters consistent. However, given the habits of the internet, we can \n",
    "    \n",
    "- I believe we can consider the second comment to be more toxic. But let's try to understand what makes it toxic: \n",
    "    - \"If you weren't so lazy and intent on harassment, you could use Google to search for \"\"Gargamel\"\" and \"\"Jew\"\"\". \n",
    "        - This particular sentence feels targeted, with lazy being the negative description.   \n",
    "        - Might want to consider pulling common words, phrases, and general patterns with the provided validation dataset as a means to start. However, keep in mind that the dataset also contains data that is not in our testing set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fe3796",
   "metadata": {},
   "source": [
    "###### Validation_data.csv\n",
    "After playing around with the data, I've come to realize the order of the comments do not matter. As described on Kaggle, this ia a collection of comments, which a person judged a pair of comments to see which one is more or less toxic (making almost binary). The comments are kinda hilarious from a third person perspective.  \n",
    "  \n",
    "Looking at the data on excel, I noticed that some comments are repeated, showing that it's been consistently rated as being more toxic than the other. It might be interesting to make a histogram based off of the number times a comment is rated as toxic. We can also create a list of toxic comments and have it ordered from most toxic to least toxic based off the number of counts it was rated as more toxic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2c87a5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "worker                                                      313\n",
       "less_toxic              This article sucks \\n\\nwoo woo wooooooo\n",
       "more_toxic    WHAT!!!!!!!!?!?!!?!?!!?!?!?!?!!!!!!!!!!!!!!!!!...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vd = pd.read_csv(\"./data/validation_data.csv\")\n",
    "eg3 = vd.loc[0,:]\n",
    "eg3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3426f918",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "worker                                                       46\n",
       "less_toxic                           I'm Jim,a retarded idiot .\n",
       "more_toxic     You are a nazi. \\n\\nYour defense of the Latin...\n",
       "Name: 100, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eg4 = vd.loc[100,:]\n",
    "eg4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794f1721",
   "metadata": {},
   "source": [
    "##### sample_submission.csv\n",
    "This simply tells us how we should make our submission, it is interesting that we only care about the comment_id and the score. Although I am more interested in how the score will be developed. Is it unique or what?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4d5b529",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_id</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>114890</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>732895</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1139051</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1434512</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2084821</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2452675</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   comment_id  score\n",
       "0      114890    0.5\n",
       "1      732895    0.5\n",
       "2     1139051    0.5\n",
       "3     1434512    0.5\n",
       "4     2084821    0.5\n",
       "5     2452675    0.5"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eg5 = pd.read_csv(\"./data/sample_submission.csv\")\n",
    "eg5.loc[0:5,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8364aa8b",
   "metadata": {},
   "source": [
    "### Testing out Ideas\n",
    "\n",
    "In the previous section, I discussed ideas to explore. In this section, we will try out those ideas and see how they could apply to our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8d5f7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Find all the unique comments in the validation dataset. \n",
    "\n",
    "# Alright, so I want to look at this data with ease. To save time and memory, I'm going to give each unique comment a unique number\n",
    "# And count the number of occurences the unique ID has. \n",
    "toxic_unique = vd.more_toxic.unique()\n",
    "toxic_dict = {};\n",
    "not_toxic_unique = vd.less_toxic.unique()\n",
    "not_toxic_dict = {};\n",
    "count = 0;\n",
    "\n",
    "for tword in toxic_unique:\n",
    "    toxic_dict[count] = [tword, len(vd.index[vd.more_toxic == tword].tolist())]\n",
    "    count += 1\n",
    "    # Create a dictionary that we can use\n",
    "    #I don't know how much time or memory this uses, but basicaly I am creating a dictionary which stores the number of counts per unique comment\n",
    "\n",
    "count = 0\n",
    "for ntword in not_toxic_unique:\n",
    "    not_toxic_dict[count] = [ntword, len(vd.index[vd.less_toxic == ntword].tolist())] # Create a dictionary that we can use\n",
    "    count +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30c651ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3603</th>\n",
       "      <td>YOU SUCK \\n\\nI HATE YOU&gt; YOU CAN FUCK MY ASS ...</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>905</th>\n",
       "      <td>STUPID SPANISH CENTRALISTSTUPID SPANISH CENTRA...</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>788</th>\n",
       "      <td>MORON! The USA has only 5% of the worlds popul...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1247</th>\n",
       "      <td>LAST WARNING YOU FUCKING TOSSER, IT TAKES ME 1...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990</th>\n",
       "      <td>Imma cut off ur nutz and send em to ur gf \\n\\...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1516</th>\n",
       "      <td>PEOPLE ON THIS SITE SUCK  \\n\\nPEOPLE ON THIS ...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4523</th>\n",
       "      <td>\"\\n\\nBastard Child\\nDoes anyone know if Obama ...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2130</th>\n",
       "      <td>INTEVIEW \\n\\nME: ARE YOU GAY?\\n\\nABE GAY DASH...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>563</th>\n",
       "      <td>YOU'RE A FUCKING LIAR WHO DOESN'T KNOW HOW TO ...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>695</th>\n",
       "      <td>ScJessey is a Big Fat Faggot Pinko Commie Asshole</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5244</th>\n",
       "      <td>go to hell \\n\\ngo to hell and leave me alone</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1993</th>\n",
       "      <td>Ok, what is it \\n\\nWhy are you such a queer.....</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2683</th>\n",
       "      <td>Why don't you just fuck off?</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4836</th>\n",
       "      <td>Missrain4, I will make a rain of cum on your f...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2760</th>\n",
       "      <td>Gonna fuck you up \\n\\nYou little bitch. 86.17...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5281</th>\n",
       "      <td>fuck you jzg \\n\\ni will have you removed as ed...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3048</th>\n",
       "      <td>Fuck you, Smith.  Please have me notified when...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7791</th>\n",
       "      <td>MOTHERFUCKER, DONT BE CHANGING THE FUCKING GTA...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2714</th>\n",
       "      <td>\"\\nNo, I'm not joking.  I'm hoping to help BD ...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2621</th>\n",
       "      <td>Stupidhead! Dweeb! How dare you say that! Scor...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  count\n",
       "3603   YOU SUCK \\n\\nI HATE YOU> YOU CAN FUCK MY ASS ...     14\n",
       "905   STUPID SPANISH CENTRALISTSTUPID SPANISH CENTRA...     14\n",
       "788   MORON! The USA has only 5% of the worlds popul...     12\n",
       "1247  LAST WARNING YOU FUCKING TOSSER, IT TAKES ME 1...     12\n",
       "1990   Imma cut off ur nutz and send em to ur gf \\n\\...     12\n",
       "1516   PEOPLE ON THIS SITE SUCK  \\n\\nPEOPLE ON THIS ...     12\n",
       "4523  \"\\n\\nBastard Child\\nDoes anyone know if Obama ...     12\n",
       "2130   INTEVIEW \\n\\nME: ARE YOU GAY?\\n\\nABE GAY DASH...     12\n",
       "563   YOU'RE A FUCKING LIAR WHO DOESN'T KNOW HOW TO ...     12\n",
       "695   ScJessey is a Big Fat Faggot Pinko Commie Asshole     12\n",
       "5244     go to hell \\n\\ngo to hell and leave me alone       12\n",
       "1993   Ok, what is it \\n\\nWhy are you such a queer.....     11\n",
       "2683                       Why don't you just fuck off?     11\n",
       "4836  Missrain4, I will make a rain of cum on your f...     11\n",
       "2760   Gonna fuck you up \\n\\nYou little bitch. 86.17...     11\n",
       "5281  fuck you jzg \\n\\ni will have you removed as ed...     11\n",
       "3048  Fuck you, Smith.  Please have me notified when...     11\n",
       "7791  MOTHERFUCKER, DONT BE CHANGING THE FUCKING GTA...     11\n",
       "2714  \"\\nNo, I'm not joking.  I'm hoping to help BD ...     11\n",
       "2621  Stupidhead! Dweeb! How dare you say that! Scor...     11"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxic_df = pd.DataFrame.from_dict(toxic_dict, orient = 'index', columns = ['text', 'count']).sort_values(by =['count'], ascending = False)\n",
    "not_toxic_df = pd.DataFrame.from_dict(not_toxic_dict, orient = 'index', columns = ['text', 'count']).sort_values(by = ['count'], ascending = False)\n",
    "\n",
    "toxic_df[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "218803f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5422</th>\n",
       "      <td>How many sockpuppets do you have?</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>786</th>\n",
       "      <td>sorry i jumped to conclusions \\n\\non christia...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2192</th>\n",
       "      <td>Sockpuppet\\nHi, would you please confirm this ...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4930</th>\n",
       "      <td>I erased your cuss word\\nFrom: some random per...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4021</th>\n",
       "      <td>An addition comment of interest by Andrew Coyn...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6612</th>\n",
       "      <td>this irishtom guy is turning every article int...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4646</th>\n",
       "      <td>Please no Wiki-Weirdo posts !</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9383</th>\n",
       "      <td>the fife shuffle \\n\\naahh dismas surely you a...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1401</th>\n",
       "      <td>\"\\n\\n Please stop your disruptive editing. If ...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4043</th>\n",
       "      <td>You are not sorry one damned bit.  You have ye...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7821</th>\n",
       "      <td>You vandalized the Flavor of Love acticle. Sto...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4229</th>\n",
       "      <td>Hello \\n\\nOK, don't try to frighten me with y...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2174</th>\n",
       "      <td>are you proud to be propaganda tool puppet of ...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>Support- stops the POV of nationalist editors</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7300</th>\n",
       "      <td>Never physical harm. Since you're directly con...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7071</th>\n",
       "      <td>this still should not stop me editing my page....</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4052</th>\n",
       "      <td>That's pretty damn cool, an old medievil town,...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6267</th>\n",
       "      <td>\" 2009 (UTC)\\n\\nWould it be possible to sugges...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>579</th>\n",
       "      <td>\"Please do not add nonsense to Wikipedia. It i...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1741</th>\n",
       "      <td>yo  \\n\\nyou could at least reply to my messag...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  count\n",
       "5422                 How many sockpuppets do you have?      13\n",
       "786    sorry i jumped to conclusions \\n\\non christia...     13\n",
       "2192  Sockpuppet\\nHi, would you please confirm this ...     12\n",
       "4930  I erased your cuss word\\nFrom: some random per...     12\n",
       "4021  An addition comment of interest by Andrew Coyn...     12\n",
       "6612  this irishtom guy is turning every article int...     12\n",
       "4646                      Please no Wiki-Weirdo posts !     11\n",
       "9383   the fife shuffle \\n\\naahh dismas surely you a...     11\n",
       "1401  \"\\n\\n Please stop your disruptive editing. If ...     11\n",
       "4043  You are not sorry one damned bit.  You have ye...     11\n",
       "7821  You vandalized the Flavor of Love acticle. Sto...     11\n",
       "4229   Hello \\n\\nOK, don't try to frighten me with y...     11\n",
       "2174  are you proud to be propaganda tool puppet of ...     11\n",
       "860    Support- stops the POV of nationalist editors        11\n",
       "7300  Never physical harm. Since you're directly con...     11\n",
       "7071  this still should not stop me editing my page....     11\n",
       "4052  That's pretty damn cool, an old medievil town,...     11\n",
       "6267  \" 2009 (UTC)\\n\\nWould it be possible to sugges...     10\n",
       "579   \"Please do not add nonsense to Wikipedia. It i...     10\n",
       "1741   yo  \\n\\nyou could at least reply to my messag...     10"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_toxic_df[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b48e721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic_df length:  11678\n",
      "not_toxic_df length:  11532\n"
     ]
    }
   ],
   "source": [
    "print(\"toxic_df length: \", len(toxic_df))\n",
    "print(\"not_toxic_df length: \", len(not_toxic_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaff0ae7",
   "metadata": {},
   "source": [
    "As you can see, I created two different dataframes to rank the frequency of comments found in the toxic and less toxic columns from the validity data provided. One thing to keep in mind is that we will want our algorithm to generalize, therefore having a keyword classifer might not be particularly useful. Instead of looking for specific words, we should think about the underlying trends in the data.      \n",
    "  \n",
    "Interestingly, in the toxic column, a lot of the comments use an abundance of capital letters. Furthermore, comments deemed more toxic seem to have more misspelling, slang, and punctuation. Of course, this type of identification is for the most obvious cases. However, this leads me to wonder if we can apply an algorithm to identify underlying trends.   \n",
    "  \n",
    "I almost glossed over one thing: The goal of this project is to rank the comments from most toxic to least toxic. As explained in the project description, it is very difficult to rank a list of comments entirely. Instead, it is a lot easier to judge two comments and rank which one is toxic. I should keep this in mind when I'm building my future algorithm. Instead of loading the entire dataset into my model, it might be better to use a sliding window method that takes in two comments and spits out which one is considered more toxic. This leads me to think that a binary classifier might be particularly useful. However, how would we identify the unlabeled features? Hmmm.... How much natural language processing is actually necessary here?\n",
    "  \n",
    "On a side note, I think it might be interesting to see the amount of overlap between the two classes (more toxic vs less toxic)\n",
    "I find it funny that there's more toxic comments than not toxic comments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6db7a961",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_df = {}\n",
    "un_not_toxic_df = {}\n",
    "\n",
    "\n",
    "count = 0\n",
    "ncount = 0\n",
    "\n",
    "for i in range(len(not_toxic_df.text)):\n",
    "    ntword = not_toxic_df.text[i]\n",
    "    ntcount = not_toxic_df['count'][i]\n",
    "\n",
    "    if ntword in list(toxic_df.text):\n",
    "        t_idx = (toxic_df.index[toxic_df.text == ntword].tolist())[0]\n",
    "        t_count = toxic_df['count'][t_idx]\n",
    "        tword = toxic_df['text'][t_idx]\n",
    "        common_df[count] = [ntword, ntcount, t_count]\n",
    "        count += 1\n",
    "        \n",
    "    else:\n",
    "        un_not_toxic_df[ncount] = [ntword, ntcount]\n",
    "        ncount += 1\n",
    "        \n",
    "common_df = pd.DataFrame.from_dict(common_df, orient = 'index', columns = ['text', 'ntcount', 'tcount'])\n",
    "un_not_toxic_df = pd.DataFrame.from_dict(un_not_toxic_df, orient = 'index', columns = ['text', 'ntcount']).sort_values(by = ['ntcount'], ascending = False)\n",
    "un_not_toxic_df = un_not_toxic_df.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "15ab27d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "un_toxic_df = {}\n",
    "\n",
    "for i in range(len(toxic_df)):\n",
    "    tword = toxic_df.text[i]\n",
    "    if tword not in list(common_df.text):\n",
    "        tcount = toxic_df['count'][i]\n",
    "        un_toxic_df[count] = [tword, tcount]\n",
    "        count += 1\n",
    "\n",
    "un_toxic_df = pd.DataFrame.from_dict(un_toxic_df, orient = 'index', columns = ['text', 'tcount']).sort_values(by = ['tcount'], ascending = False)\n",
    "un_toxic_df = un_toxic_df.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b4b1ffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   text  ntcount  tcount\n",
      "302   STUPID SPANISH CENTRALISTSTUPID SPANISH CENTRA...        1      14\n",
      "7223   YOU SUCK \\n\\nI HATE YOU> YOU CAN FUCK MY ASS ...        1      14\n",
      "299   ScJessey is a Big Fat Faggot Pinko Commie Asshole        3      12\n",
      "6370   Imma cut off ur nutz and send em to ur gf \\n\\...        3      12\n",
      "759   \"\\nNo, I'm not joking.  I'm hoping to help BD ...        4      11\n",
      "1116  Fuck you, Smith.  Please have me notified when...        1      11\n",
      "2973   Gonna fuck you up \\n\\nYou little bitch. 86.17...        1      11\n",
      "3768  Missrain4, I will make a rain of cum on your f...        1      11\n",
      "4623  MOTHERFUCKER, DONT BE CHANGING THE FUCKING GTA...        1      11\n",
      "4791                      |decline=Fucking bellend â€”           1      11\n",
      "\n",
      "shared comments count:  8959\n",
      "toxic comments total:  11678\n",
      "less toxic comments total:  11532\n",
      "% of overlap for toxic:  76.7\n",
      "% of overlap for less toxic:  77.7\n"
     ]
    }
   ],
   "source": [
    "common_df = common_df.sort_values(by = ['tcount', 'ntcount'], ascending = False)\n",
    "print(common_df[0:10])\n",
    "print(\"\\nshared comments count: \", len(common_df))\n",
    "print(\"toxic comments total: \", len(toxic_df))\n",
    "print(\"less toxic comments total: \", len(not_toxic_df))\n",
    "print(\"% of overlap for toxic: \", (round(len(common_df)/len(toxic_df),3)*100))\n",
    "print(\"% of overlap for less toxic: \", (round(len(common_df)/len(not_toxic_df),3)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc25366",
   "metadata": {},
   "source": [
    "This is interesting... There is a huge overlap between our more toxic and less toxic comments. Furthermore, the percentage of overlap is similar in both cases, with 76.7% overlap for toxic and 77.7% for less toxic which makese sense given that both datasets are similar sizes. This also tells me that close to 25% of each dataset is uniquely toxic and not toxic, which is also worth exploring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3bca938e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uniquely Toxic Comments, length:  2719\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tcount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>go to hell \\n\\ngo to hell and leave me alone</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LAST WARNING YOU FUCKING TOSSER, IT TAKES ME 1...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"\\n\\nBastard Child\\nDoes anyone know if Obama ...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>YOU'RE A FUCKING LIAR WHO DOESN'T KNOW HOW TO ...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>INTEVIEW \\n\\nME: ARE YOU GAY?\\n\\nABE GAY DASH...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>MORON! The USA has only 5% of the worlds popul...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>PEOPLE ON THIS SITE SUCK  \\n\\nPEOPLE ON THIS ...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>suck my dick irish guy you faggot</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>YOU ARE A FUCKING NERD. GO KILL YOURSELF YOU N...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>You dummasses, I DID SAY THE DAMN RIGHT STUFF...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  tcount\n",
       "0     go to hell \\n\\ngo to hell and leave me alone        12\n",
       "1  LAST WARNING YOU FUCKING TOSSER, IT TAKES ME 1...      12\n",
       "2  \"\\n\\nBastard Child\\nDoes anyone know if Obama ...      12\n",
       "3  YOU'RE A FUCKING LIAR WHO DOESN'T KNOW HOW TO ...      12\n",
       "4   INTEVIEW \\n\\nME: ARE YOU GAY?\\n\\nABE GAY DASH...      12\n",
       "5  MORON! The USA has only 5% of the worlds popul...      12\n",
       "6   PEOPLE ON THIS SITE SUCK  \\n\\nPEOPLE ON THIS ...      12\n",
       "7                  suck my dick irish guy you faggot       9\n",
       "8  YOU ARE A FUCKING NERD. GO KILL YOURSELF YOU N...       9\n",
       "9   You dummasses, I DID SAY THE DAMN RIGHT STUFF...       9"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Uniquely Toxic Comments, length: \", len(un_toxic_df))\n",
    "un_toxic_df[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b2a165cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uniquely Not Toxic Comments, length:  2573\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>ntcount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>An addition comment of interest by Andrew Coyn...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sockpuppet\\nHi, would you please confirm this ...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>, anything else inserted here will be removed</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ru-sib \\n\\nThe number of bots increases. You ...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\".\\n\\n\"\"This emphasis also led Boas to conclud...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\"\\n\\n The above argument \\n\\nI just discerned ...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>\"\\nSahih Hadith are only second in authority a...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>How is that vandalism? \\n\\nI'm the most recen...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>BTW \\n\\nI've noticed your page, and with all ...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>If you want to post rubbish about Meyer Moran ...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  ntcount\n",
       "0  An addition comment of interest by Andrew Coyn...       12\n",
       "1  Sockpuppet\\nHi, would you please confirm this ...       12\n",
       "2      , anything else inserted here will be removed        9\n",
       "3   ru-sib \\n\\nThe number of bots increases. You ...        9\n",
       "4  \".\\n\\n\"\"This emphasis also led Boas to conclud...        9\n",
       "5  \"\\n\\n The above argument \\n\\nI just discerned ...        9\n",
       "6  \"\\nSahih Hadith are only second in authority a...        9\n",
       "7   How is that vandalism? \\n\\nI'm the most recen...        9\n",
       "8   BTW \\n\\nI've noticed your page, and with all ...        9\n",
       "9  If you want to post rubbish about Meyer Moran ...        9"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Uniquely Not Toxic Comments, length: \", len(un_not_toxic_df))\n",
    "un_not_toxic_df[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd54025",
   "metadata": {},
   "source": [
    "I believe looking at this new data supports what I was saying previously, which is that all caps comments are a great way to identify toxic comments. However, as I said before, this will only help to discern the most obvious cases. This remains true with regarding less toxic comments and proper grammar. Later, as I build my model, I should keep in mind ways to check for proper spelling and grammar. \n",
    "\n",
    "If I think about this in larger scenario, should I create my own scoring method. This might help with unsupervised learning. For example, I could add points based off of the ratio of capital words to total words, which will push all my capital words to the most toxic part of my list...\n",
    "\n",
    "\n",
    "### Update 1/12/2022\n",
    "- If I had to think about what type of algorithm I should use, it would be something like semi-supervised or unsupervised. However, now I need to think about how I would like to preprocess my data... Furthermore, I should be using my validation as exactly that; Validation data.... Maybe I could take a part of my validation data and use that as a training set and validate that afterwards? I'll need to double check on that....\n",
    "     \n",
    "\n",
    "\n",
    "### Let's discuss potential algorithms to use: \n",
    "\n",
    "###### Naive Bayes / SVM\n",
    "- Supervised Learning method. It'll require me to know what features I'm looking for. I discussed potential features above, which include the amount of misspelled words, typos, swears, capitalized letters, and punctuation.\n",
    "     - One way to prove the validity of this statement is to create some histograms. \n",
    "          - For example, capital letters: Each bin will contain different ratios of total capitalized letters to total amount of words. The Y axis is the frequency of counts per toxic or not toxic.       \n",
    "            \n",
    "            \n",
    "- However, my particular issue with Naive Baye's in concept is that this only takes into features that I can tell right off the bat. What about features that I don't know about? \n",
    "\n",
    "##### Unsupervised Learning Methods (Deep Learning)\n",
    "- The biggest benefit here is the ability to extract unknown features. Since this is considered an NLP project, CNN and RNN models are considered, although I'm fairly certain that CNN is used to identify images or handwritten digits. This leads me to think that creating an RNN model through Keras might be the best foot going forward, but I will need to read more about this to get a better understanding. Plus, I believe this area will require more preproccessing of my data, which I'm still not sure how to go about it\n",
    "\n",
    "\n",
    "##### Other Notes: \n",
    "I was scouring the kaggle sites, looking at work other people have done. I noticed that a good portion of them use the Ridge Regression model, which I don't yet understand why...\n",
    "- This is a model tuning method, which uses L2 regularization.... I'll need to figure how I might utilize this later. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0a547f",
   "metadata": {},
   "source": [
    "## Naive Baye's\n",
    "### Building the Histogram\n",
    "First, let's think about separating our data. We should have a training set, validation set, and testing set, which we will take from our validiation.csv. \n",
    "  \n",
    "  \n",
    "In the training set, we will build our histograms and get some probabilities. From what I've observed in other people's code; they try to clean the data by removing special characters. As I stated previously, I don't think I agree with this. A very 'naive' way to go about this is to build the model off text only. \n",
    "\n",
    "  \n",
    "p(CapitalizedWords|Toxic) * p(mispelledwords|Toxic) * p(punctuation|toxic) * p(swears|toxic)  \n",
    "p(CapitalizedWords|not toxic) * p(mispelledwords|not toxic) * p(punctuation|not toxic) * p(swears|not toxic)  \n",
    "  - But wait! These features seem to mainly lean towards toxic. What about features that are noticable for less toxic?  \n",
    "    - Honestly I don't see anything that is... distinguishable... It's basically proper punctuation and properly spelled words.\n",
    "    - Potential issues:\n",
    "        - We shouldn't forget, that we also don't know about ranking... How will we score it?\n",
    "        - Remember earlier, when I was talking about how this might only detect the most obvious cases? What about cases that aren't so obvious, like a rude passive aggressive comment that displays proper grammar? This might just be room for error, but still..... You know what whatever! Let's build a beta version first\n",
    "  \n",
    "  \n",
    "Preprocessing my data:   \n",
    "Luckily for me, I started preprocessing my data, which are toxic_df and not_toxic_df. As stated previously, these datasets are pretty balanced. Actually, as I think about it, I can just pull random datapoints from my unique datasets and my common datasets and then appending them, given that the distribution is pretty balanced.   \n",
    "It might be worth interesting to treat each punctuation (specifically exclamation points) as entire words. \n",
    "\n",
    "\n",
    "#### There might also be a dataset that could be helpful, which is a kaggle provided dataset that rated comments based off of severity.\n",
    "- Another naive baye's classifier that I saw utilized this to train. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b8aff175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        sorry i jumped to conclusions \\n\\non christia...\n",
      "1                      How many sockpuppets do you have? \n",
      "2       this irishtom guy is turning every article int...\n",
      "3       I erased your cuss word\\nFrom: some random per...\n",
      "4       You are not sorry one damned bit.  You have ye...\n",
      "                              ...                        \n",
      "8954    unblock me, i didnt even do anything, my dumb ...\n",
      "8955     Wow... \\n\\nThis page is so awful. It has some...\n",
      "8956     ANI \\n\\nYou Have NO RIGHT, NO RIGHT TO undo A...\n",
      "8957                              eat shit an die faggot.\n",
      "8958     Outrageous!!!!! \\n\\nThis block is outrageous ...\n",
      "Name: text, Length: 8959, dtype: object\n",
      "                                                   text  ntcount  tcount\n",
      "0      sorry i jumped to conclusions \\n\\non christia...       13       5\n",
      "1                    How many sockpuppets do you have?        13       2\n",
      "2     this irishtom guy is turning every article int...       12       6\n",
      "3     I erased your cuss word\\nFrom: some random per...       12       3\n",
      "4     You are not sorry one damned bit.  You have ye...       11       7\n",
      "...                                                 ...      ...     ...\n",
      "8954  unblock me, i didnt even do anything, my dumb ...        1       2\n",
      "8955   Wow... \\n\\nThis page is so awful. It has some...        1       2\n",
      "8956   ANI \\n\\nYou Have NO RIGHT, NO RIGHT TO undo A...        1       2\n",
      "8957                            eat shit an die faggot.        1       2\n",
      "8958   Outrageous!!!!! \\n\\nThis block is outrageous ...        1       2\n",
      "\n",
      "[8959 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "temp = common_df.sort_values(by = ['ntcount', 'tcount'], ascending = False);\n",
    "temp = temp.reset_index(drop = True)\n",
    "print(temp.text)\n",
    "print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3b54c94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_col(df, col_name, col_num):\n",
    "    scol = np.zeros([1,len(df)])[0]\n",
    "    df.insert(col_num, col_name, scol.tolist(), True)\n",
    "    return df    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f5470d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "un_toxic_df = zero_col(un_toxic_df, 'ntcount', 1)\n",
    "un_not_toxic_df = zero_col(un_not_toxic_df, 'tcount', 2)\n",
    "\n",
    "train_t, valid_t, test_t = np.split(un_toxic_df.sample(frac = 1, random_state = 42), [int(.6*len(un_toxic_df)), int(.8*len(un_toxic_df))])\n",
    "train_nt, valid_nt, test_nt = np.split(un_not_toxic_df.sample(frac = 1, random_state = 42), [int(.6*len(un_not_toxic_df)), int(.8*len(un_not_toxic_df))])\n",
    "train_c, valid_c, test_c = np.split(common_df.sample(frac = 1, random_state = 42), [int(.6*len(common_df)), int(.8*len(common_df))])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0336ab",
   "metadata": {},
   "source": [
    "#### Constructing Frequency Table:  \n",
    "Naive Baye's works best with binary data, i.e True or False. However, our results aren't entirely binary (See common_df). Sometimes we have mixed results, with 14 votes saying something is toxic and 3 votes saying something isn't. This is something we should take into account... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6a41c2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_training = pd.concat([train_t, train_nt, common_df], axis = 0)\n",
    "full_training = full_training.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9c1b5ef9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text       . PS. FUCK U BITCH GO EAT A DICK HO. PROBABLY ...\n",
       "ntcount                                                  0.0\n",
       "tcount                                                   4.0\n",
       "Name: 1, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_training.loc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "748e0bac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". PS. FUCK U BITCH GO EAT A DICK HO. PROBABLY DILDO URSELF TO SEAN KINGSTON'S ALBUM COVER.\n"
     ]
    }
   ],
   "source": [
    "test = full_training.text[1]\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "53e38933",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\n",
      "\n",
      "The article explains a manifestation of antisemitism; it is not, as stated above, \"\"a defence.\"\" The assertion regarding \"\"idiotic eating practices\"\" is manifestly not NPOV; and, approval of other certifications, i.e., \"\"Certified organic, FDA approved[,] etc[.],\"\" shows an inconsistency that perforce implies a false premiss. talk \"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>ntcount</th>\n",
       "      <th>tcount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"\\n\\nThe article explains a manifestation of a...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>. PS. FUCK U BITCH GO EAT A DICK HO. PROBABLY ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FUCK YOU, NUNH-HUH. 69.45.178.143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>You also want eternal hell???\\n\\nâ€˜chaitanya-ma...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Because my name is Yalma, and I am a \\n\\n== ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  ntcount  tcount\n",
       "0  \"\\n\\nThe article explains a manifestation of a...      0.0     3.0\n",
       "1  . PS. FUCK U BITCH GO EAT A DICK HO. PROBABLY ...      0.0     4.0\n",
       "2                FUCK YOU, NUNH-HUH. 69.45.178.143        0.0     3.0\n",
       "3  You also want eternal hell???\\n\\nâ€˜chaitanya-ma...      0.0     3.0\n",
       "4    Because my name is Yalma, and I am a \\n\\n== ...      0.0     3.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(full_training.text[0])\n",
    "full_training.iloc[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "422d7526",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "test_punct = re.findall(r\"''+|[^\\w\\s]\", test, re.UNICODE)\n",
    "test_words = (re.sub(\"[^\\w\\s]\", \"\", test)).split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c0fb5ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import enchant\n",
    "def mspcount(words):\n",
    "    d = enchant.Dict(\"en_US\")\n",
    "    word_count = 0\n",
    "    for word in words:\n",
    "        if d.check(word) is False: word_count +=1\n",
    "    return ((word_count+1)/(len(words)+1))\n",
    "\n",
    "def capital_check(words):\n",
    "    word_count = 0\n",
    "    for word in words:\n",
    "        if word.isupper() is True: word_count +=1\n",
    "        return(word_count/len(words))\n",
    "    \n",
    "def freq(df):\n",
    "    toxic_freq = []\n",
    "    not_toxic_freq = []\n",
    "    for index, row in df.iterrows():\n",
    "        #test_punct = round(re.findall(r\"''+|[^\\w\\s]\", row.text, re.UNICODE),4) #Test this out later\n",
    "        test_words = (re.sub(\"[^\\w\\s]\", \"\", row.text)).split()\n",
    "        \n",
    "        ## Arbitrary Test...\n",
    "        if capital_check(row.text) != 0:\n",
    "            if row.tcount > row.ntcount:\n",
    "                toxic_freq.append(capital_check(row.text))\n",
    "            else: \n",
    "                not_toxic_freq.append(capital_check(row.text))\n",
    "        \n",
    "    return (np.asarray(toxic_freq), np.asarray(not_toxic_freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f88264fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text       IS REALLY GAY LOLZOZLZ \n",
      "ntcount                        3.0\n",
      "tcount                         0.0\n",
      "Name: 1742, dtype: object\n",
      "text       From DDMC: Just shut up.\n",
      "ntcount                         3.0\n",
      "tcount                          0.0\n",
      "Name: 1845, dtype: object\n",
      "text       Did you just block me? \n",
      "ntcount                        6.0\n",
      "tcount                         0.0\n",
      "Name: 1867, dtype: object\n",
      "text       YOU WILL DIE IN 7 DAYS.\n",
      "ntcount                        3.0\n",
      "tcount                         0.0\n",
      "Name: 2052, dtype: object\n",
      "text       Oh, bugger off, Kate! \n",
      "ntcount                       3.0\n",
      "tcount                        0.0\n",
      "Name: 2091, dtype: object\n"
     ]
    }
   ],
   "source": [
    "test_count = 0\n",
    "for index, row in full_training.iterrows():\n",
    "    if row.tcount > row.ntcount:\n",
    "        continue\n",
    "    else:\n",
    "        if capital_check(row.text) > .04:\n",
    "            test_count += 1\n",
    "            print(row)\n",
    "            if test_count == 5:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "23fdfbda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mm = \"Oh\"\n",
    "mm.isupper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "57f8f6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttable,nttable = freq(full_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f73994dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.025     , 0.01190476, 0.00666667, ..., 0.00657895, 0.00934579,\n",
       "       0.00092678])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nttable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "642273ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.02857143, 0.00259067, 0.01666667, ..., 0.00403226, 0.00304878,\n",
       "       0.02439024])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ttable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "08062afc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.025     , 0.01190476, 0.00666667, ..., 0.00657895, 0.00934579,\n",
       "       0.00092678])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nttable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a30f92f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.007194244604316547"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.median(ttable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a318dc80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist([ttable, nttable],density = True, bins = 20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "be61c5c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(nttable,density = True, bins = 20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f863306e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfreq, tbin = np.histogram(ttable, bins = 20, range = [0,1])\n",
    "ntfreq, ntbin = np.histogram(nttable, bins = 20, range = [0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ec9c31",
   "metadata": {},
   "source": [
    "### Updates\n",
    "So, as you can see from my histogram, my blanket idea is flawed. My method at the moment doesn't take into account the different lengths of each comment, which skews my distributions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c32d272",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
